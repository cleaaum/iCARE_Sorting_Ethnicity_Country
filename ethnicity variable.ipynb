{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95679887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries. You will  most likely need to install: JUPYTER, PANDAS, FUZZYWUZZY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "from dictionaries.ethnicity_dictionary import ethnicity_dictionary\n",
    "from dictionaries.ethnicity_granularity_2 import granularity2_dictionary\n",
    "from dictionaries.ethnicity_granularity_3 import granularity3_dictionary\n",
    "from adjust_ethnicites import adjust_ethnicites\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "pd.set_option(\"display.max_rows\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49861e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data into a dataframe (a type of data structure)\n",
    "#if you get errors, make sure the file is a csv, try changing the \"sep=\" to a comma \",\".\n",
    "df_ethn = pd.read_csv('ethnicity_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to normalize the data detected. This means first capitalizing the string and removing all punctuation. \n",
    "# However we have to be careful here. Blindly removing punctuation will remove ALL special characters, including \n",
    "# chinese characters, arab scripts etc. Regular accents tend to have an ascii number lower than 900, so this \n",
    "#differentiates accents from different writings/scripts.\n",
    "def normalize_data(country):\n",
    "#     if type(country) is float: return # find better way to check for this\n",
    "    country_upper_case = country.upper().strip()\n",
    "    country_removed_punctuation = re.sub(\"[-|_|:|·|.|,|\\\"|\\“|\\”|/|+|(|)|?|'|’|&|)|(|?|!|%|*|·]\", \" \", country_upper_case)\n",
    "    for char in country_removed_punctuation:\n",
    "        if ord(char) < 900:\n",
    "            continue\n",
    "        else:\n",
    "            return country_removed_punctuation\n",
    "    country_removed_accents = unicodedata.normalize('NFD', country_removed_punctuation).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return country_removed_accents.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02db9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to dynamically reverse the dictionnaries to increase run time. This is only stored in memory \n",
    "# during the run time of the program. Easier to look up a key in a dict then to loop through all the values \n",
    "def reverse_dict(dict_to_reverse):\n",
    "    reversed_dict = defaultdict(list)\n",
    "    for key, value in dict_to_reverse.items():\n",
    "        for val in value:\n",
    "            reversed_dict[val] = (key)\n",
    "        reversed_dict[key] = (key)\n",
    "    return reversed_dict\n",
    "# 3 dictionnaries we need (all reversed)\n",
    "ethn_rev = reverse_dict(ethnicity_dictionary)\n",
    "ethn2_rev = reverse_dict(granularity2_dictionary)\n",
    "ethn3_rev = reverse_dict(granularity3_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b85de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the repsonse to look up each word individually (to allow for multiple ethnicites)\n",
    "def find_ethnicity(ethnicity):\n",
    "    ethn_split = ethnicity.split()\n",
    "    ethn_split.append(ethnicity)\n",
    "    ethnicites = []\n",
    "    for ethn in ethn_split:\n",
    "        if ethn in ethn_rev.keys():\n",
    "            ethnicites.append(ethn_rev.get(ethn))\n",
    "    ethnicites = list(set(ethnicites))\n",
    "    if len(ethnicites) is 0 or 'MISSING' in ethnicites: return np.nan\n",
    "    return np.array(ethnicites, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(ethn_matched, ethn_normalized):\n",
    "    if type(ethn_matched) is not float: return ethn_matched\n",
    "    ethn_split = ethn_normalized.split()\n",
    "    if len(ethn_split) > 1: ethn_split.append(ethn_normalized)\n",
    "    ethnicities = []\n",
    "    for ethn in ethn_split:\n",
    "        for key in ethnicity_dictionary.keys():\n",
    "            ratio = fuzz.ratio(ethn, key)\n",
    "            if ratio >= 92:\n",
    "                ethnicities.append(ethn_rev.get(key))\n",
    "    if len(ethnicities) == 0: return np.nan\n",
    "    return np.array(list(set(ethnicities)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ethnicity_granularity_2(ethnicities_found):\n",
    "    if type(ethnicities_found) is float: return ethnicities_found\n",
    "    ethnicities_found = ethnicities_found.tolist()\n",
    "    ethnicity_granularity_2 = []\n",
    "    for ethn in ethnicities_found:\n",
    "        ethn_matched = ethn2_rev.get(ethn)\n",
    "        if ethn_matched is not None:\n",
    "            ethnicity_granularity_2.append(ethn_matched)\n",
    "    return np.array(list(set(ethnicity_granularity_2)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ethnicity_granularity_3(ethnicities_found):\n",
    "    if type(ethnicities_found) is float: return ethnicities_found\n",
    "    ethnicities_found = ethnicities_found.tolist()\n",
    "    ethnicity_granularity_3 = []\n",
    "    for ethn in ethnicities_found:\n",
    "        ethn_matched = ethn3_rev.get(ethn)\n",
    "        if ethn_matched is not None:\n",
    "            ethnicity_granularity_3.append(ethn_matched)\n",
    "    return np.array(list(set(ethnicity_granularity_3)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fae2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn = df_ethn[~df_ethn['ethn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff982616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['ethn_normalized'] = np.vectorize(normalize_data)(df_ethn['ethn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['ethn_matched'] = np.vectorize(find_ethnicity)(df_ethn['ethn_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65667bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['ethn_fuzzy_matched'] = np.vectorize(fuzzy_match)(df_ethn['ethn_matched'],df_ethn['ethn_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c29497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['ethn_adjusted'] = np.vectorize(adjust_ethnicites)(df_ethn['ethn_fuzzy_matched'],df_ethn['ethn_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['gran_2'] = np.vectorize(find_ethnicity_granularity_2)(df_ethn['ethn_adjusted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f110d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ethn['gran_3'] = np.vectorize(find_ethnicity_granularity_3)(df_ethn['gran_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the extra columns we created that are needed for the final dataset\n",
    "df_ethn = df_ethn.drop(['ethn_matched'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column with the normalizedc country name.\n",
    "df_ethn = df_ethn.rename(columns={'ethn_adjusted':'gran_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b08c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our dataframe to a csv file (this can be to another format as well such as SAS, excel).\n",
    "# This will be in the same directory as the code and inital dataset.\n",
    "df_country.to_csv('ethnicity_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2b05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the list of countries that were not categorized. Usually gibberish.\n",
    "# If there is a name that should have been detected, add it to the dictionnary under the right country.\n",
    "unclassified_ethnicities = df_ethn.loc[df_ethn['gran_1'].isna()]\n",
    "unclassified_ethnicities_list = unclassified_ethnicities['ethn_normalized'].tolist()\n",
    "print(\"There are a total of\", len(list(set(unclassified_ethnicities_list))), \n",
    "      \" different unclassified Responses and \", len(unclassified_ethnicities_list), \n",
    "      \" unclassified total responses...\")\n",
    "print(list(set(unclassified_ethnicities_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
